{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np \nimport pandas as pd\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import dstack\nfrom numpy import concatenate\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport keras\nimport tensorflow as tf\nimport scipy.stats as stats\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\nimport tensorflow as tf\nimport keras.backend.tensorflow_backend as tfback\nfrom keras.layers import  Reshape\nfrom keras.layers import Conv2D, MaxPooling2D\ndef _get_available_gpus():\n    \"\"\"Get a list of available gpu devices (formatted as strings).\n\n    # Returns\n        A list of available GPU devices.\n    \"\"\"\n    #global _LOCAL_DEVICES\n    if tfback._LOCAL_DEVICES is None:\n        devices = tf.config.list_logical_devices()\n        tfback._LOCAL_DEVICES = [x.name for x in devices]\n    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n\ntfback._get_available_gpus = _get_available_gpus\n# from keras import backend as K\n# K.tensorflow_backend._get_available_gpus()\n# config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n# sess = tf.Session(config=config) \n# keras.backend.set_session(sess)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/smartphone-and-smartwatch-activity-and-biometrics/wisdm-dataset/wisdm-dataset/raw/')\n\n# %% [code]\n\n# load a single file as a numpy array\ndef load_file(filepath):\n    dataframe = read_csv(filepath,sep = ',', header=None)\n    return dataframe.to_numpy()\n \n# load a list of files and return as a 3d numpy array\ndef load_group(filenames, prefix=''):\n    loaded = list()\n    for name in filenames:\n        data = load_file(prefix + name)\n        loaded.append(data)\n    # stack group so that features are the 3rd dimension\n    loaded = concatenate(loaded)\n    return loaded\n \n# load a dataset group, such as train or test\ndef load_dataset_group(sample='', prefix='',sensor='',recorder=''):\n#     sensor = 'accel'\n#     recorder = 'phone'\n    filepath = recorder +'/'+ sensor +'/'\n    # load all 9 files as a single array\n    filenames = list()\n    # total acceleration\n   \n    filenames += ['data_'+sample+'_'+sensor+'_'+recorder+'.txt']\n            \n    for filename in filenames:\n        df = load_file(filepath+filename)\n    return df\n \n# load the dataset, returns train and test X and y elements\ndef load_dataset(prefix=''):\n    # load all train\n    prefix = ''\n    dataSampleTrain = list()\n    recorder = ['phone','watch']\n    sensor  = ['accel','gyro']\n    for i in range (1600,1610):\n        size = 99999\n        data = list()\n        for rec in recorder:\n            for sens in sensor:\n                DS = load_dataset_group(str(i), prefix,sens,rec)\n                size = min(size,DS.shape[0])\n                data.append(DS)\n        \n        \n            \n        for i in range(4):\n            data[i] = data[i][:size]\n        dataSampleTrain.append(concatenate(data, axis = 1))\n        \n    \n    dataSampleTrain = concatenate(dataSampleTrain)\n    Traindf = pd.DataFrame(dataSampleTrain)\n    #Traindf.to_csv('TrainDF.csv')\n    \n    dataSampleTest = list() \n    for i in range (1620,1634):\n        size = 99999\n        data = list()\n        for rec in recorder:\n            for sens in sensor:\n                DS = load_dataset_group(str(i), prefix,sens,rec)\n                size = min(size,DS.shape[0])\n                data.append(DS)\n        \n        \n            \n        for i in range(4):\n            data[i] = data[i][:size]\n        dataSampleTest.append(concatenate(data, axis = 1))\n        \n    \n    dataSampleTest = concatenate(dataSampleTest)\n    Testdf = pd.DataFrame(dataSampleTest)\n    #Traindf.to_csv('TrainDF.csv')\n    \n    return Traindf,Testdf\ntrainDS,testDS = load_dataset('')\ntrainDS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess and load X\ndef X_pre(trainDS):\n    \n    \n    cols = [5,11,17,23]\n    for col in cols:\n            trainDS[trainDS.columns[col]] = trainDS[trainDS.columns[col]].str.replace(';','').astype(float)\n    \n    columns = ['x','y','z']\n    X = pd.DataFrame(data=None, columns=columns)\n    X['x'] = trainDS[3]+trainDS[9]+trainDS[15]+trainDS[21]\n    X['y'] = trainDS[4]+trainDS[10]+trainDS[16]+trainDS[22]\n    X['z'] = trainDS[5]+trainDS[11]+trainDS[17]+trainDS[23]\n    \n    # train the normalization\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    return X\n\ntrainX = X_pre(trainDS)\ntestX = X_pre(testDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess and load Y\n\ndef Y_pre(trainDS):\n    cols = [1,7,13,19]\n    Y = np.zeros((trainDS.shape[0]),dtype=int)\n\n    lable_encoder = LabelEncoder()\n\n    pos = list()\n    for col in cols:\n        y = trainDS[col]\n        y = lable_encoder.fit_transform(y)\n        pos.append(y)\n\n    pos = np.vstack((pos))\n\n    for i in range (trainDS.shape[0]):\n        Y[i] = np.bincount(pos[:,i]).argmax()\n\n    return Y\n\ntrainY = Y_pre(trainDS)\ntestY = Y_pre(testDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fs=20\nframe_size = Fs*4 #80\nhop_size = Fs*2 #40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_frames(df,tag, frame_size, hop_size):\n    \n    N_FEATURES = df.shape[1]\n    frames = []\n    labels = []\n    for i in range(0,len(df )- frame_size, hop_size):\n        x1 = df[i: i+frame_size,0]\n        y1 = df[i: i+frame_size,1]\n        z1 = df[i: i+frame_size,2]\n        \n        label = stats.mode(tag[i: i+frame_size])[0][0]\n        frames.append([x1,y1,z1])\n        labels.append(label)\n        \n    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n    labels = np.asarray(labels)\n    \n    return frames, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/device:GPU:0'):\n    train_X,train_Y = get_frames(trainX,trainY, frame_size, hop_size)\n    test_X,test_Y = get_frames(testX,testY, frame_size, hop_size)\n    trainx = train_X.reshape(train_X.shape[0], 80, 3,1)\n    testx = test_X.reshape(test_X.shape[0], 80, 3,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train,y_train,x_test,y_test = train_X,train_Y,test_X,test_Y\nwith tf.device('/device:GPU:0'):\n    #batch_size = 12\n    model = Sequential()\n    model.add(Conv2D(256, (2,2), activation = 'relu', input_shape = x_train[0].shape))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(512, (2,2), activation = 'relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n\n    model.add(Dense(1024, activation = 'relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(18, activation='softmax'))\n    \n    model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) \n    history = model.fit(x_train, y_train, epochs = 25, validation_data=(x_test, y_test), verbose=1 )\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}